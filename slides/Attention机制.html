<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Attention机制</title>
    <link rel="shortcut icon" href="./favicon.ico"/>
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./dist/theme/solarized.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

## Attention Is All You Need


</script></section><section  data-markdown><script type="text/template">
### 目录

1. 循环神经网络
2. 注意力机制
3. Transformer模型
4. BERT模型

<aside class="notes"><ul>
<li>首先我想介绍一下为什么我们需要循环神经网络</li>
<li>然后介绍如何在循环神经网络中加入注意力机制</li>
<li>接下来我会介绍Transformer模型的细节, 以及它是如何做到Attention is all you need.</li>
<li>最后, 我会在Transformer的基础上简要的介绍一下BERT模型</li>
</ul>
</aside></script></section><section ><section data-markdown><script type="text/template">
### 前馈神经网络


![传统的神经网络结构](images/bert-1.jpg)
</script></section><section data-markdown><script type="text/template">

- 网络结构固定
- 两次计算过程相互独立

<aside class="notes"><ul>
<li>由输入层, 若干隐含层和一个输出层构成</li>
<li>网络结构固定对于长度不定的语言来说是一个缺陷, 自然语言文本的长度是不确定的.</li>
<li>计算过程独立, 自然语言文本的每个词与周围的词都存在的一定的关系, 前馈神经网络无法很好的捕捉这一点</li>
</ul>
</aside></script></section></section><section ><section data-markdown><script type="text/template">
### 循环神经网络


![按时间展开的循环神经网络](images/bert-2.jpg)

<aside class="notes"><ul>
<li>横向箭头表示按照时间展开</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">

![完整的循环神经网络](images/bert-3.jpg)
</script></section></section><section ><section data-markdown><script type="text/template">
### 编码解码框架

![Encode-Decode](images/bert-4.jpg)

- 输入中文, 输出英文 --> 翻译系统
- 输入文章, 输出摘要 --> 文本摘要系统
- 输入问题, 输出回答 --> 问答系统

</script></section><section data-markdown><script type="text/template">

![RNN进行翻译的问题](images/bert-6.jpg)

- 将所有信息压缩成固定长度的中间向量
- 输入与输出之间没有明确的对应关系
- 运算过程相互依赖, 无法并行加速


<aside class="notes"><ul>
<li>如果网络比较小, 训练数据少, 这也不算是一个问题, 但如果向扩大模型的规模, 那么无法并行加速就导致网络的规模受到限制</li>
</ul>
</aside></script></section></section><section ><section data-markdown><script type="text/template">
### Attention机制


![Attention机制的简单解释](images/bert-6.jpg)

<aside class="notes"><ul>
<li>每次输入都有一个中间表示</li>
<li>每次都可以注意某一部分信息</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
![Attention机制的简单解释](images/bert-7.jpg)

</script></section><section data-markdown><script type="text/template">
![权重关系图](images/bert-9.jpg)

<aside class="notes"><ul>
<li>每个词都可以关注输入的任何部分, 解决了信息损失的问题</li>
<li>提供了一定的可解释性</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
Attention机制就是给输入分配权重

</script></section></section><section ><section data-markdown><script type="text/template">
### Attention机制的数学表示

$$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$$
</script></section><section data-markdown><script type="text/template">
![Attention机制的简单解释](images/bert-8.jpg)

$$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$$


<aside class="notes"><p>点乘只是最简单的一种方式, 不一定是点乘</p>
</aside></script></section></section><section  data-markdown><script type="text/template">
### Self-Attention机制

![Attention机制的简单解释](images/bert-8.jpg)

$$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$$
</script></section><section ><section data-markdown><script type="text/template">

### Transformer模型

![Transformer宏观结构](images/bert-10.jpg)
</script></section><section data-markdown><script type="text/template">
![Transformer架构图](images/bert-11.jpg)

</script></section><section data-markdown><script type="text/template">
![Transformer宏观架构图](images/bert-12.jpg)
</script></section><section data-markdown><script type="text/template">
![Encoder内部的结构](images/bert-13.jpg)
</script></section><section data-markdown><script type="text/template">
![Encoder的并行计算](images/bert-14.jpg)


</script></section></section><section ><section data-markdown><script type="text/template">
### Transformer计算过程

![注意力机制计算过程](images/bert-15.jpg)

</script></section><section data-markdown><script type="text/template">
![注意力机制计算过程](images/bert-16.jpg)
</script></section><section data-markdown><script type="text/template">
![image](images/bert-17.jpg)
</script></section><section data-markdown><script type="text/template">
![image](images/bert-18.jpg)

</script></section></section><section ><section data-markdown><script type="text/template">
### Transformer的多头机制

![Transformer的多头机制](images/bert-19.jpg)

</script></section><section data-markdown><script type="text/template">
![多头机制输出的输出](images/bert-20.jpg)
</script></section><section data-markdown><script type="text/template">
![多头机制输出的合并](images/bert-21.jpg)
</script></section><section data-markdown><script type="text/template">
![多头机制的完整计算过程](images/bert-22.jpg)
</script></section></section><section ><section data-markdown><script type="text/template">

### 残差连接和归一化

![全局结构](images/bert-24.jpg)
</script></section><section data-markdown><script type="text/template">
![残差连接和归一化](images/bert-23.jpg)
</script></section></section><section ><section data-markdown><script type="text/template">
### Decoder结构

![全局结构](images/bert-24.jpg)

</script></section><section data-markdown><script type="text/template">
![全局计算过程](images/bert-25-2.gif)
</script></section><section data-markdown><script type="text/template">
![全局计算过程](images/bert-25.gif)
</script></section></section><section  data-markdown><script type="text/template">
### 位置编码

![全局结构](images/bert-24.jpg)

<aside class="notes"><p>抛弃了RNN架构的同时, 获得了可以并行计算的好处, 但同时又失去了输入之间相互关联的好处, 从而引入位置编码</p>
</aside></script></section><section ><section data-markdown><script type="text/template">
### 两个问题

1. 如何理解Self-Attention
2. Transformer能不能并行计算

</script></section><section data-markdown><script type="text/template">


![Self-Attention自动识别代词](images/bert-28.jpg)

- 上下文向量

<aside class="notes"><p>Amazing, 上下文向量, 卷积, 底层的网络抽取细节, 高层的网络抽取语义</p>
</aside></script></section><section data-markdown><script type="text/template">
#### Teacher Forcing

![Transformer架构图](images/bert-11.jpg)
</script></section></section><section ><section data-markdown><script type="text/template">

### BERT模型

BERT: Bidirectional Encoder Representations from Transformers.


![image](images/bert-26.jpg)

</script></section><section data-markdown><script type="text/template">
![image](images/bert-27.jpg)
</script></section></section><section  data-markdown><script type="text/template">
### 个人感受


1. Attention机制解决了信息遗忘问题
2. 位置嵌入机制解决了词语顺序信息的问题
3. Teacher Frocing技术解决了训练并行化问题
4. Self Attention提取了上下文信息
5. 多方面的获取资源: 论文, 博客, 知乎, 视频等

  </script></section><section  data-markdown><script type="text/template">
### 参考资料

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- [Self-Attention与Transformer](https://zhuanlan.zhihu.com/p/47282410)
- [More About Attention](https://zhuanlan.zhihu.com/p/106662375)</script></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"transition":"fade"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
